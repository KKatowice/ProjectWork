{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping dati per creazione dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ricaviamo i dettagli tecnici di ogni versione di ogni modello di ogni marca di auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creazione classe per scrape html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeGenerator:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = None\n",
    "\n",
    "    async def scrape_data(self):\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            }\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.url,headers=headers) as response:\n",
    "                    content = await response.text()\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    self.soup = soup\n",
    "                    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per prendere i modelli in produzione di ogni marca di auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_car_brand_data(sp: BeautifulSoup):\n",
    "    # lista quantita auto in produzione\n",
    "    bi = [y.text for y in sp.find_all('b',{'class':'col-green2'})]\n",
    "    #trova tutti gli h5 che contengono gli 'a' con href al modello\n",
    "    sp = sp.find_all('h5')\n",
    "    all_brands_links = []\n",
    "    for x in tqdm(range(len(sp)),\"scraping car brand data\"):\n",
    "        if bi[x] == '0':\n",
    "            continue\n",
    "        #aggiungi tutti i link che hanno modelli in production\n",
    "        all_brands_links.append(sp[x].find('a',href=True)['href'])\n",
    "\n",
    "    return all_brands_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per ricavare i modelli di ogni marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPresent(l: list):\n",
    "    for x in l:\n",
    "        if \"present\" in str(x).lower():\n",
    "            return True\n",
    "def removeBrandName(s: str, torem: str):\n",
    "    if \"-\" in torem:\n",
    "        torem = torem.replace(\"-\",\" \")\n",
    "    return s.replace(torem,\"\").strip()\n",
    "\n",
    "async def scrape_car_model_data(lstBrand: list):\n",
    "    ret = {} #{ brand:{model:{..info}}, ...  }\n",
    "    for link_brand in tqdm(lstBrand,\"scraping car models\"):\n",
    "        nme = link_brand.split(\"/\")[3]\n",
    "        sg = ScrapeGenerator(link_brand)\n",
    "        sg = await sg.scrape_data()\n",
    "        imglink = sg.find_all('div',{'class':'pic'})\n",
    "        imglink = imglink[0].find('img')['src']\n",
    "        ret[nme] = {'imglink':imglink}\n",
    "        divCars = sg.find_all('div',{'class':'carmod clearfix'})\n",
    "        #prendo ogni div che contiene ogni macchina\n",
    "        for car in divCars:\n",
    "            isPrnt = isPresent(car.find_all('span'))\n",
    "            if not isPrnt:\n",
    "                continue\n",
    "            #se e' ancora in production aggiungiamo il link alla lista\n",
    "            mname = removeBrandName(car.h4.text, nme.upper())\n",
    "            lnk = car.a['href']\n",
    "            ret[nme][mname] = {'link':lnk}\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per ricavare ogni modello di auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEngines(divvoneCar: BeautifulSoup, brand: str, model: str):\n",
    "    boxEngines = divvoneCar.find_all('div',{'class':'mot clearfix'})\n",
    "    eng = {}\n",
    "    for x in boxEngines:\n",
    "        for y in x.find_all('a'):\n",
    "            eng[y.text] = {'link':y['href']}\n",
    "    return eng\n",
    "    \n",
    "async def scrape_eachModel_data(dizModels: dict):\n",
    "    for brand in tqdm(dizModels.keys(),f\"scraping models details\"):\n",
    "        for model in dizModels[brand]:\n",
    "            if model == \"imglink\": continue\n",
    "            sg = ScrapeGenerator(dizModels[brand][model]['link'])\n",
    "            sg = await sg.scrape_data()\n",
    "            \n",
    "            imgcar = sg.find('div',{'class':'col1width fl'})\n",
    "            imgcar = imgcar.find('img')['src']\n",
    "        \n",
    "            divEngines = sg.find_all('div',{'class':'container carmodel clearfix'})\n",
    "            engList = getEngines(divEngines[0],brand,model)\n",
    "            dizModels[brand][model]['engines'] = engList\n",
    "            dizModels[brand][model]['car_imglink'] = imgcar\n",
    "\n",
    "    return dizModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per ricavare i dettagli di ogni versione di ogni modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_eachEngine_data(dizModels: dict):\n",
    "    for brand in tqdm(dizModels.keys(),f\"scraping engines details\"):\n",
    "        for model in dizModels[brand]:\n",
    "            if model == \"imglink\": continue\n",
    "            for engine in dizModels[brand][model]['engines']:\n",
    "                sg = ScrapeGenerator(dizModels[brand][model]['engines'][engine]['link'])#['link']\n",
    "                sg = await sg.scrape_data()\n",
    "                tableInfo = sg.find_all('div',{'class':'padcol2'})\n",
    "                for y in tableInfo:\n",
    "                    if \"ENGINE SPECS\" in y.text:\n",
    "                        tableInfo = y\n",
    "                        break\n",
    "                try:\n",
    "                    descInfo = tableInfo.find_all('td',{'class':'left'})\n",
    "                    descVal = tableInfo.find_all('td',{'class':'right'})\n",
    "                except:\n",
    "                    print(f\"{tableInfo}\")\n",
    "\n",
    "                pattern = r'\\((\\d+)\\sHP\\)'\n",
    "                match = re.search(pattern, engine)\n",
    "                if match:\n",
    "                    dizModels[brand][model]['engines'][engine][\"HP\"] = match.group(1)\n",
    "                else:\n",
    "                    dizModels[brand][model]['engines'][engine][\"HP\"] = None\n",
    "                \n",
    "                pattern = r'\\d+(?:\\.\\d+)?(?:cc|L)'\n",
    "                match = re.search(pattern, engine)\n",
    "                if match:\n",
    "                    dizModels[brand][model]['engines'][engine][\"Pdisplacement\"] = match.group(0)\n",
    "                else:\n",
    "                    dizModels[brand][model]['engines'][engine][\"Pdisplacement\"] = None\n",
    "\n",
    "                for x in range(len(descInfo)):\n",
    "                    dizModels[brand][model]['engines'][engine][descInfo[x].text] = descVal[x].text.strip()\n",
    "    with open('completo.json', 'w') as f:\n",
    "        json.dump(dizModels, f)\n",
    "    return dizModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione principale che esegue i vari passaggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(url):\n",
    "    sg = ScrapeGenerator(url)\n",
    "    sg = await sg.scrape_data()\n",
    "    lbrand = await scrape_car_brand_data(sg)\n",
    "    dcarModel = await scrape_car_model_data(lbrand)\n",
    "    eachModelData = await scrape_eachModel_data(dcarModel)\n",
    "    scrape_eachEngine_data(eachModelData)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = \"https://www.autoevolution.com/cars/\"\n",
    "    asyncio.run(main(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ricaviamo i prezzi tramite ricerca google e scraping del risultato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iniziliziamo variabili globali e set delle opzioni dei webdriver di chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver_path = \"/usr/bin/google-chrome\"\n",
    "pathFile = \"Datasets_Scraping/completo_wPrices_official.json\"\n",
    "pathInitFile = \"Datasets_Scraping/completo.json\"\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.binary_location = chrome_driver_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cerchiamo nell html della pagina della ricerca di google un prezzo coerente tramite regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_integer(number_str):\n",
    "    number_str =  number_str.replace(\"USD\",\"\").replace(\"$\",\"\").replace(\"EUR\",\"\").replace(\"€\",\"\").replace(\"£\",\"\").replace(\" \",\"\").strip()\n",
    "    pattern = r'\\.\\d{2}$' \n",
    "    pattern2 = r',\\d{2}$'  \n",
    "    if re.search(pattern, number_str):\n",
    "        number_str = number_str.split(\".\")[0]\n",
    "    elif re.search(pattern2, number_str):\n",
    "        number_str = number_str.split(\",\")[0]\n",
    "    return int(number_str.replace(',', '').replace('.', ''))\n",
    "\n",
    "async def seleniumscrape(link):\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "    browser.get(link)\n",
    "    acc = WebDriverWait(browser, 100).until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='L2AGLb']\")))\n",
    "    acc.click()\n",
    "    element = WebDriverWait(browser, 10).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='main']\")))\n",
    "\n",
    "    reg = r'((?:\\$|€|USD)\\s*\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2})?|\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2})?\\$|\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2})?\\s*(?:\\$|€|USD))' #TODO check if regex works, added ?\n",
    "    fnda = re.findall(reg, element.text) \n",
    "    numbers = []\n",
    "    try:\n",
    "        for num in fnda:\n",
    "            numbers.append(convert_to_integer(num))\n",
    "        max_number = max(numbers)\n",
    "        if max_number > 20000000:\n",
    "            numbers.remove(max_number)\n",
    "            max_number = max(numbers)\n",
    "        elif max_number < 2000:\n",
    "            max_number = 0\n",
    "    except Exception as e:\n",
    "        max_number = 0\n",
    "        print(f\"error \\n{e}\\nON {link}\")\n",
    "    finally:\n",
    "        browser.quit()\n",
    "    return max_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione principale che esegue i vari passaggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    with open(pathInitFile, 'r') as f:\n",
    "        dcarModel = json.load(f)\n",
    "        for x in tqdm(dcarModel, f\"scraping prices\"):\n",
    "            for y in dcarModel[x]:\n",
    "                if y == \"imglink\": continue\n",
    "                lnk = f'https://www.google.com/search?q={x}+{y}+price'\n",
    "                pricez = await seleniumscrape(lnk)\n",
    "                dcarModel[x][y]['price'] = pricez\n",
    "        with open(pathFile, 'w') as f:\n",
    "            json.dump(dcarModel, f)\n",
    "            \n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puliamo i dati del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iniziliziamo variabili globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathFile = \"Datasets_Scraping/completo_wPrices_cleaner_official.json\"\n",
    "pathInitFile = \"Datasets_Scraping/completo_wPrices_official.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Puliamo il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDs():\n",
    "    with open(pathInitFile, 'r') as f:\n",
    "        dcarModel = json.load(f)\n",
    "        dcarModel_cln = copy.deepcopy(dcarModel)\n",
    "        deletatiByCilindr = 0\n",
    "        deletatiByEng = 0\n",
    "        deletatiByPrc = 0\n",
    "        for brand in tqdm(dcarModel.keys()):\n",
    "            for model in dcarModel[brand].keys():\n",
    "                if model == 'imglink': continue\n",
    "                a = dcarModel[brand][model]\n",
    "                if a['price'] == 0:\n",
    "                    del dcarModel_cln[brand][model]\n",
    "                    deletatiByPrc += 1\n",
    "                    continue\n",
    "                \n",
    "                if len(a['engines'].keys()) == 0:\n",
    "                    print(\"cancello [eng == 0]\",brand, model)\n",
    "                    del dcarModel_cln[brand][model]\n",
    "                    deletatiByEng += 1\n",
    "                    continue\n",
    "                for eng in a['engines'].keys():\n",
    "                    if len(a['engines'][eng].keys()) <3:\n",
    "                        print(\"cancello [eng < 3]\",brand, model, eng)\n",
    "                        del dcarModel_cln[brand][model]['engines'][eng]\n",
    "                        deletatiByEng += 1\n",
    "                        continue\n",
    "                \n",
    "                    disp = a['engines'][eng].get('Displacement:')\n",
    "                    pdisp = a['engines'][eng].get('Pdisplacement')\n",
    "                    if disp == None and pdisp == None:\n",
    "                        print(\"cancello [no cilindr info]\",brand, model, eng)\n",
    "                        del dcarModel_cln[brand][model]['engines'][eng]\n",
    "                        deletatiByCilindr += 1\n",
    "                    elif pdisp == None and disp:\n",
    "                        numz =  \"{:.1f}\".format(int(disp.split()[0])/1000) \n",
    "                        dcarModel_cln[brand][model]['engines'][eng]['Pdisplacement'] = f\"{numz}L\"\n",
    "                        \n",
    "                if len(dcarModel_cln[brand][model]['engines'].keys()) == 0:\n",
    "                    print(\"cancello [eng == 0]\",brand, model)\n",
    "                    del dcarModel_cln[brand][model]\n",
    "                    deletatiByEng += 1\n",
    "                    continue\n",
    "                \n",
    "        print(\"cancellatiByEngine\", deletatiByEng)\n",
    "        print(\"cancellatiByCilindri\", deletatiByCilindr)\n",
    "        print(\"cancellatiByPrezzo\", deletatiByPrc)\n",
    "                    \n",
    "    with open(pathFile, 'w') as f:\n",
    "        json.dump(dcarModel_cln, f)\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    cleanDs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
