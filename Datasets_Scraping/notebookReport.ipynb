{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping dati per creazione dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ricaviamo i dettagli tecnici di ogni versione di ogni modello di ogni marca di auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creazione classe per scrape html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapeGenerator:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.soup = None\n",
    "\n",
    "    async def scrape_data(self):\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "            }\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(self.url,headers=headers) as response:\n",
    "                    content = await response.text()\n",
    "                    soup = BeautifulSoup(content, 'html.parser')\n",
    "                    self.soup = soup\n",
    "                    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per prendere i modelli in produzione di ogni marca di auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_car_brand_data(sp: BeautifulSoup):\n",
    "    # lista quantita auto in produzione\n",
    "    bi = [y.text for y in sp.find_all('b',{'class':'col-green2'})]\n",
    "    #trova tutti gli h5 che contengono gli 'a' con href al modello\n",
    "    sp = sp.find_all('h5')\n",
    "    all_brands_links = []\n",
    "    for x in tqdm(range(len(sp)),\"scraping car brand data\"):\n",
    "        if bi[x] == '0':\n",
    "            continue\n",
    "        #aggiungi tutti i link che hanno modelli in production\n",
    "        all_brands_links.append(sp[x].find('a',href=True)['href'])\n",
    "\n",
    "    return all_brands_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per ricavare i modelli di ogni marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPresent(l: list):\n",
    "    for x in l:\n",
    "        if \"present\" in str(x).lower():\n",
    "            return True\n",
    "def removeBrandName(s: str, torem: str):\n",
    "    if \"-\" in torem:\n",
    "        torem = torem.replace(\"-\",\" \")\n",
    "    return s.replace(torem,\"\").strip()\n",
    "\n",
    "async def scrape_car_model_data(lstBrand: list):\n",
    "    ret = {} #{ brand:{model:{..info}}, ...  }\n",
    "    for link_brand in tqdm(lstBrand,\"scraping car models\"):\n",
    "        nme = link_brand.split(\"/\")[3]\n",
    "        sg = ScrapeGenerator(link_brand)\n",
    "        sg = await sg.scrape_data()\n",
    "        imglink = sg.find_all('div',{'class':'pic'})\n",
    "        imglink = imglink[0].find('img')['src']\n",
    "        ret[nme] = {'imglink':imglink}\n",
    "        divCars = sg.find_all('div',{'class':'carmod clearfix'})\n",
    "        #prendo ogni div che contiene ogni macchina\n",
    "        for car in divCars:\n",
    "            isPrnt = isPresent(car.find_all('span'))\n",
    "            if not isPrnt:\n",
    "                continue\n",
    "            #se e' ancora in production aggiungiamo il link alla lista\n",
    "            mname = removeBrandName(car.h4.text, nme.upper())\n",
    "            lnk = car.a['href']\n",
    "            ret[nme][mname] = {'link':lnk}\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per ricavare ogni modello di auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEngines(divvoneCar: BeautifulSoup, brand: str, model: str):\n",
    "    boxEngines = divvoneCar.find_all('div',{'class':'mot clearfix'})\n",
    "    eng = {}\n",
    "    for x in boxEngines:\n",
    "        for y in x.find_all('a'):\n",
    "            eng[y.text] = {'link':y['href']}\n",
    "    return eng\n",
    "    \n",
    "async def scrape_eachModel_data(dizModels: dict):\n",
    "    for brand in tqdm(dizModels.keys(),f\"scraping models details\"):\n",
    "        for model in dizModels[brand]:\n",
    "            if model == \"imglink\": continue\n",
    "            sg = ScrapeGenerator(dizModels[brand][model]['link'])\n",
    "            sg = await sg.scrape_data()\n",
    "            \n",
    "            imgcar = sg.find('div',{'class':'col1width fl'})\n",
    "            imgcar = imgcar.find('img')['src']\n",
    "        \n",
    "            divEngines = sg.find_all('div',{'class':'container carmodel clearfix'})\n",
    "            engList = getEngines(divEngines[0],brand,model)\n",
    "            dizModels[brand][model]['engines'] = engList\n",
    "            dizModels[brand][model]['car_imglink'] = imgcar\n",
    "\n",
    "    return dizModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione per ricavare i dettagli di ogni versione di ogni modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_eachEngine_data(dizModels: dict):\n",
    "    for brand in tqdm(dizModels.keys(),f\"scraping engines details\"):\n",
    "        for model in dizModels[brand]:\n",
    "            if model == \"imglink\": continue\n",
    "            for engine in dizModels[brand][model]['engines']:\n",
    "                sg = ScrapeGenerator(dizModels[brand][model]['engines'][engine]['link'])#['link']\n",
    "                sg = await sg.scrape_data()\n",
    "                tableInfo = sg.find_all('div',{'class':'padcol2'})\n",
    "                for y in tableInfo:\n",
    "                    if \"ENGINE SPECS\" in y.text:\n",
    "                        tableInfo = y\n",
    "                        break\n",
    "                try:\n",
    "                    descInfo = tableInfo.find_all('td',{'class':'left'})\n",
    "                    descVal = tableInfo.find_all('td',{'class':'right'})\n",
    "                except:\n",
    "                    print(f\"{tableInfo}\")\n",
    "\n",
    "                pattern = r'\\((\\d+)\\sHP\\)'\n",
    "                match = re.search(pattern, engine)\n",
    "                if match:\n",
    "                    dizModels[brand][model]['engines'][engine][\"HP\"] = match.group(1)\n",
    "                else:\n",
    "                    dizModels[brand][model]['engines'][engine][\"HP\"] = None\n",
    "                \n",
    "                pattern = r'\\d+(?:\\.\\d+)?(?:cc|L)'\n",
    "                match = re.search(pattern, engine)\n",
    "                if match:\n",
    "                    dizModels[brand][model]['engines'][engine][\"Pdisplacement\"] = match.group(0)\n",
    "                else:\n",
    "                    dizModels[brand][model]['engines'][engine][\"Pdisplacement\"] = None\n",
    "\n",
    "                for x in range(len(descInfo)):\n",
    "                    dizModels[brand][model]['engines'][engine][descInfo[x].text] = descVal[x].text.strip()\n",
    "    with open('completo.json', 'w') as f:\n",
    "        json.dump(dizModels, f)\n",
    "    return dizModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione principale che esegue i vari passaggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(url):\n",
    "    sg = ScrapeGenerator(url)\n",
    "    sg = await sg.scrape_data()\n",
    "    lbrand = await scrape_car_brand_data(sg)\n",
    "    dcarModel = await scrape_car_model_data(lbrand)\n",
    "    eachModelData = await scrape_eachModel_data(dcarModel)\n",
    "    scrape_eachEngine_data(eachModelData)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = \"https://www.autoevolution.com/cars/\"\n",
    "    asyncio.run(main(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ricaviamo i prezzi tramite ricerca google e scraping del risultato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iniziliziamo variabili globali e set delle opzioni dei webdriver di chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_driver_path = \"/usr/bin/google-chrome\"\n",
    "pathFile = \"Datasets_Scraping/completo_wPrices_official.json\"\n",
    "pathInitFile = \"Datasets_Scraping/completo.json\"\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.binary_location = chrome_driver_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cerchiamo nell html della pagina della ricerca di google un prezzo coerente tramite regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_integer(number_str):\n",
    "    number_str =  number_str.replace(\"USD\",\"\").replace(\"$\",\"\").replace(\"EUR\",\"\").replace(\"€\",\"\").replace(\"£\",\"\").replace(\" \",\"\").strip()\n",
    "    pattern = r'\\.\\d{2}$' \n",
    "    pattern2 = r',\\d{2}$'  \n",
    "    if re.search(pattern, number_str):\n",
    "        number_str = number_str.split(\".\")[0]\n",
    "    elif re.search(pattern2, number_str):\n",
    "        number_str = number_str.split(\",\")[0]\n",
    "    return int(number_str.replace(',', '').replace('.', ''))\n",
    "\n",
    "async def seleniumscrape(link):\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "    browser.get(link)\n",
    "    acc = WebDriverWait(browser, 100).until(EC.element_to_be_clickable((By.XPATH, \"//button[@id='L2AGLb']\")))\n",
    "    acc.click()\n",
    "    element = WebDriverWait(browser, 10).until(EC.visibility_of_element_located((By.XPATH, \"//div[@class='main']\")))\n",
    "\n",
    "    reg = r'((?:\\$|€|USD)\\s*\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2})?|\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2})?\\$|\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d{2})?\\s*(?:\\$|€|USD))' #TODO check if regex works, added ?\n",
    "    fnda = re.findall(reg, element.text) \n",
    "    numbers = []\n",
    "    try:\n",
    "        for num in fnda:\n",
    "            numbers.append(convert_to_integer(num))\n",
    "        max_number = max(numbers)\n",
    "        if max_number > 20000000:\n",
    "            numbers.remove(max_number)\n",
    "            max_number = max(numbers)\n",
    "        elif max_number < 2000:\n",
    "            max_number = 0\n",
    "    except Exception as e:\n",
    "        max_number = 0\n",
    "        print(f\"error \\n{e}\\nON {link}\")\n",
    "    finally:\n",
    "        browser.quit()\n",
    "    return max_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funzione principale che esegue i vari passaggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    with open(pathInitFile, 'r') as f:\n",
    "        dcarModel = json.load(f)\n",
    "        for x in tqdm(dcarModel, f\"scraping prices\"):\n",
    "            for y in dcarModel[x]:\n",
    "                if y == \"imglink\": continue\n",
    "                lnk = f'https://www.google.com/search?q={x}+{y}+price'\n",
    "                pricez = await seleniumscrape(lnk)\n",
    "                dcarModel[x][y]['price'] = pricez\n",
    "        with open(pathFile, 'w') as f:\n",
    "            json.dump(dcarModel, f)\n",
    "            \n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puliamo i dati del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iniziliziamo variabili globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathFile = \"Datasets_Scraping/completo_wPrices_cleaner_official.json\"\n",
    "pathInitFile = \"Datasets_Scraping/completo_wPrices_official.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Puliamo il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDs():\n",
    "    with open(pathInitFile, 'r') as f:\n",
    "        dcarModel = json.load(f)\n",
    "        dcarModel_cln = copy.deepcopy(dcarModel)\n",
    "        deletatiByCilindr = 0\n",
    "        deletatiByEng = 0\n",
    "        deletatiByPrc = 0\n",
    "        for brand in tqdm(dcarModel.keys()):\n",
    "            for model in dcarModel[brand].keys():\n",
    "                if model == 'imglink': continue\n",
    "                a = dcarModel[brand][model]\n",
    "                if a['price'] == 0:\n",
    "                    del dcarModel_cln[brand][model]\n",
    "                    deletatiByPrc += 1\n",
    "                    continue\n",
    "                \n",
    "                if len(a['engines'].keys()) == 0:\n",
    "                    print(\"cancello [eng == 0]\",brand, model)\n",
    "                    del dcarModel_cln[brand][model]\n",
    "                    deletatiByEng += 1\n",
    "                    continue\n",
    "                for eng in a['engines'].keys():\n",
    "                    if len(a['engines'][eng].keys()) <3:\n",
    "                        print(\"cancello [eng < 3]\",brand, model, eng)\n",
    "                        del dcarModel_cln[brand][model]['engines'][eng]\n",
    "                        deletatiByEng += 1\n",
    "                        continue\n",
    "                \n",
    "                    disp = a['engines'][eng].get('Displacement:')\n",
    "                    pdisp = a['engines'][eng].get('Pdisplacement')\n",
    "                    if disp == None and pdisp == None:\n",
    "                        print(\"cancello [no cilindr info]\",brand, model, eng)\n",
    "                        del dcarModel_cln[brand][model]['engines'][eng]\n",
    "                        deletatiByCilindr += 1\n",
    "                    elif pdisp == None and disp:\n",
    "                        numz =  \"{:.1f}\".format(int(disp.split()[0])/1000) \n",
    "                        dcarModel_cln[brand][model]['engines'][eng]['Pdisplacement'] = f\"{numz}L\"\n",
    "                        \n",
    "                if len(dcarModel_cln[brand][model]['engines'].keys()) == 0:\n",
    "                    print(\"cancello [eng == 0]\",brand, model)\n",
    "                    del dcarModel_cln[brand][model]\n",
    "                    deletatiByEng += 1\n",
    "                    continue\n",
    "                \n",
    "        print(\"cancellatiByEngine\", deletatiByEng)\n",
    "        print(\"cancellatiByCilindri\", deletatiByCilindr)\n",
    "        print(\"cancellatiByPrezzo\", deletatiByPrc)\n",
    "                    \n",
    "    with open(pathFile, 'w') as f:\n",
    "        json.dump(dcarModel_cln, f)\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    cleanDs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cerchiamo correlazioni tra i dati che consentano di fare previsioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dati e creazione dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per comodità espositiva di questo notebook abbiamo creato un csv a partire dai dati inseriti nel database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dati_auto.csv')\n",
    "\n",
    "# Controlliamo l'eventuale presenza di valori \"nan\"\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Non ci sono valori nan ma alcuni valori numerici sono \"object\" invece che \"int\" o \"float\". Provvediamo a risolvere:\n",
    "\n",
    "df['prezzo'] = pd.to_numeric(df['prezzo'])\n",
    "df['cavalli'] = pd.to_numeric(df['cavalli'])\n",
    "df['cilindrata'] = pd.to_numeric(df['cilindrata'])\n",
    "df['consumi'] = pd.to_numeric(df['consumi'])\n",
    "df['emissioni'] = pd.to_numeric(df['emissioni'])\n",
    "df['potenza'] = pd.to_numeric(df['potenza'])\n",
    "df['serbatoio'] = pd.to_numeric(df['serbatoio'])\n",
    "\n",
    "# Restano due campi non numerici che non avranno effetto nel misurare la correlazione con una regressione lineare,\n",
    "# quindi eliminiamo queste due colonne\n",
    "\n",
    "df = df.drop(['nome', 'carburante'], axis=1)\n",
    "\n",
    "# Infine notiamo che i dati hanno unità di misura diverse e scale assai differenti, decidiamo quindi di normalizzarli\n",
    "\n",
    "colonne = df.columns\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(df)\n",
    "df = pd.DataFrame(data, columns=colonne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prima di vedere le correlazioni analizziamo i dati da un punto di vista visivo tramite grafici delle librerie matplotlib e seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniziamo con una serie di scatterplot che mostrino le relazioni tra il prezzo e ciascuna delle altre variabili\n",
    "\n",
    "fig, axes = plt.subplots(3,3 , figsize=(24,18))\n",
    "fig.tight_layout(pad=5.0)\n",
    "axes = axes.flatten()\n",
    "features = df.columns[1:]\n",
    "for i in range(len(features)):\n",
    "    sns.scatterplot(data=df, x=features[i], y=\"prezzo\", ax=axes[i])\n",
    "    plt.xlabel(str(features[i]))\n",
    "    plt.ylabel('Prezzo')\n",
    "plt.show()\n",
    "\n",
    "# Notiamo da subito che i dati appaiono abbastanza sparsi e disordinati sul piano. Già da qui possiamo inferire che ci sia un\n",
    "# basso indice di correlazione, ma ce ne sinceriamo con una heatmap:\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Heatmap della Correlazione')\n",
    "plt.show()\n",
    "\n",
    "# Completare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressione Lineare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tentiamo ora di applicare i modelli di regressione lineare di scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo le variabili target e training e attraverso train_test_split generiamo le variabili per il training del modello e \n",
    "# quelle per il test\n",
    "\n",
    "target = df['prezzo']\n",
    "training = df.drop('prezzo', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(training, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ora istanziamo il modello di regressione lineare, lo addestriamo, facciamo delle predizioni e le visualizziamo insieme all'r^2\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "preds=model.predict(X_test)\n",
    "preds\n",
    "r2_score(y_test, preds)\n",
    "\n",
    "# Osserviamo che otteniamo un r^2 negativo: questo avviene per via del tipo di calcolo effettuato da scikit learn, diverso\n",
    "# da quello della normale regressione lineare. In ogni caso ci indica, come ci aspettavamo che i dati in nostro possesso\n",
    "# apparentemente non sono in grado di predire i prezzi delle auto.\n",
    "\n",
    "# Otteniamo risulati simili anche con altri modelli\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "preds=model.predict(X_test)\n",
    "r2_score(y_test, preds)\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "preds=model.predict(X_test)\n",
    "r2_score(y_test, preds)\n",
    "\n",
    "# KNeighbors Regressor\n",
    "\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=2)\n",
    "model.fit(X_train,y_train)\n",
    "preds=model.predict(X_test)\n",
    "r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzione di regressione custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A questo punto abbiamo pensato, dato che scikit learn usa algoritmi peculiari, di sviluppare una funzione per effettuare una regressione lineare come da statistica scolastica e verificare se cambiassero i risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione che prende due liste di training e una di test e restituisce la lista delle predizioni, la correlazione r e il \n",
    "#coefficiente di determinazione r^2\n",
    "from statistics import mean\n",
    "from math import sqrt, pow\n",
    "\n",
    "def predittore(x_train, y_train, x_test): # Accetta solo liste di numeri\n",
    "    if not isinstance(x_train, list):\n",
    "        raise TypeError(\"Accetta solo una lista per x_train\")\n",
    "    if not isinstance(y_train, list):\n",
    "        raise TypeError(\"Accetta solo una lista per y_train\")\n",
    "    if not isinstance(x_test, list):\n",
    "        raise TypeError(\"Accetta solo una lista per x_test\")\n",
    "    for x in x_train:\n",
    "        if not isinstance(x, (int, float)):\n",
    "            raise TypeError(\"Ogni elemento di x_train deve essere un numero\")\n",
    "    for y in y_train:\n",
    "        if not isinstance(y, (int, float)):\n",
    "            raise TypeError(\"Ogni elemento deve di y_train essere un numero\")\n",
    "    for x in x_test:\n",
    "        if not isinstance(x, (int, float)):\n",
    "            raise TypeError(\"Ogni elemento deve di x_test essere un numero\")\n",
    "    x_medio = mean(x_train)\n",
    "    y_medio = mean(y_train)\n",
    "    lista_scarti_x = []\n",
    "    lista_scarti_y = []\n",
    "    for x in x_train:\n",
    "        scarto = x - x_medio\n",
    "        lista_scarti_x.append(scarto)\n",
    "    for y in y_train:\n",
    "        scarto = y - y_medio\n",
    "        lista_scarti_y.append(scarto)\n",
    "    prodotto_scarti = []\n",
    "    for i in range(len(lista_scarti_x)):\n",
    "        prodotto = lista_scarti_x[i] * lista_scarti_y[i]\n",
    "        prodotto_scarti.append(prodotto)\n",
    "    b = sum(prodotto_scarti)/pow(sum(lista_scarti_x), 2)\n",
    "    a = y_medio - (b*x_medio)\n",
    "    lista_predizioni = []\n",
    "    for x in x_test:\n",
    "        y = a + (b * x)\n",
    "        lista_predizioni.append(y)\n",
    "    quadrato_scarti_x = []\n",
    "    quadrato_scarti_y = []\n",
    "    for x in lista_scarti_x:\n",
    "        quadrato = pow(x, 2)\n",
    "        quadrato_scarti_x.append(quadrato)\n",
    "    for y in lista_scarti_y:\n",
    "        quadrato = pow(y, 2)\n",
    "        quadrato_scarti_y.append(quadrato)\n",
    "    s_x = sqrt((sum(quadrato_scarti_x)/(len(quadrato_scarti_x)-1)))\n",
    "    s_y = sqrt((sum(quadrato_scarti_y)/(len(quadrato_scarti_y)-1)))\n",
    "    r = (s_x / s_y) * b\n",
    "    r = round(r)\n",
    "    r_quadro = pow(r, 2)\n",
    "    return [lista_predizioni, r, r_quadro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dal dataframe estraiamo le liste dei prezzi e delle due variabili che correlano maggiormente (cavalli e potenza),\n",
    "# le splittiamo per ottenere dati di training e dati di test e infine facciamo la regressione lineare\n",
    "\n",
    "prezzo_y = df['prezzo'].tolist()\n",
    "cavalli_x = df['cavalli'].tolist()\n",
    "potenza_x = df['potenza'].tolist()\n",
    "\n",
    "\n",
    "prezzo_y_train = prezzo_y[:int((0.8*len(prezzo_y)))]\n",
    "prezzo_y_test = prezzo_y[int((0.8*len(prezzo_y))):]\n",
    "cavalli_x_train = cavalli_x[:int(0.8 * len(cavalli_x))]\n",
    "cavalli_x_test = cavalli_x[int(0.8 * len(cavalli_x)):]\n",
    "\n",
    "predittore(cavalli_x_train, prezzo_y_train, cavalli_x_test)\n",
    "\n",
    "potenza_x_train = potenza_x[:int(0.8 * len(potenza_x))]\n",
    "potenza_x_test = potenza_x[int(0.8 * len(potenza_x)):]\n",
    "\n",
    "predittore(potenza_x_train, prezzo_y_train, potenza_x_test)\n",
    "\n",
    "# Sfortunatamente notiamo valori fuori scala e una correlazione inesistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressione polinomiale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Poiché tuttavia i nostri valori, come visto dai grafici, mal si adattano a una relazione lineare tentiamo come ultima spiaggia una regressione polinomiale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importiamo il costruttore\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizziamo una polinomiale di secondo grado per evitare l'overfitting\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(training)\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, target, test_size=0.2, random_state=42)\n",
    "poly_reg_model = LinearRegression()\n",
    "poly_reg_model.fit(X_train, y_train)\n",
    "poly_reg_y_predicted = poly_reg_model.predict(X_test)\n",
    "print(r2_score(y_test, poly_reg_y_predicted))\n",
    "\n",
    "# Il modello polinomiale è molto più efficiente, ma ci restituisce comunque un modello la cui bontà di adattamento è davvero\n",
    "# bassa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
